# -*- coding: utf-8 -*-
"""Predictive Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wl61SjHRnnu9Sr8KnXRhSGMfs3Z6AMmq

# Download Datasets

Tutorial download kaggle dataset melalui google colab

https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/
"""

! pip install kaggle

from google.colab import files
uploaded = files.upload()

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download sid321axn/gold-price-prediction-dataset

! unzip gold-price-prediction-dataset.zip

"""# Import Library

"""

# Import all required libraries
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib
import matplotlib.ticker as ticker
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# Importing layers from keras. Use LSTM for input layer, and Dense for hidden and output layer
from keras.layers import Dense, LSTM, Dropout
from keras.models import Sequential

# Import for splitting test and training data set
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import seaborn as sns

# Import Adam Optimizers
from tensorflow.keras.optimizers import Adam


from keras.callbacks import  EarlyStopping

"""# Data Understanding

Pada Submission ini, dataset diambil dari [Kaggle](https://www.kaggle.com/) dengan nama dataset [Gold Price Prediction Dataset](https://www.kaggle.com/datasets/sid321axn/gold-price-prediction-dataset)
"""

df = pd.read_csv('/content/FINAL_USO.csv', parse_dates=True, squeeze=True)
df = df[['Date', 'Open', 'High', 'Low','Close','Adj Close', 'Volume']]
df.head()

print("Total Data : {} \n".format(len(df)))
print("Date range from : {} to {}".format(df.head(1)['Date'].values, df.tail(1)['Date'].values))

df.info()

df_new = df.dropna(how='any',axis=0) 
df_new

df_new.describe()

df_new['Date'] = pd.to_datetime(df_new['Date'] , format='%Y-%m-%d')
df_new['Date']

visual_plot =df_new[['Date','Close', 'Open', 'High']]

plt.figure(figsize=(20,10))

sns.lineplot(y=visual_plot['Open'], color="c", x=visual_plot['Date'])
sns.lineplot(y=visual_plot['Close'], color="m", x=visual_plot['Date'])
sns.lineplot(y=visual_plot['High'], color="y", x=visual_plot['Date'])

plt.xlabel('Tahun', fontsize=20)
plt.ylabel('Harga (Dolar)', fontsize=20)
plt.legend(['Open','Close','High'], loc='upper right')

corr = df.corr()
sns.heatmap(data=corr, annot=True, cmap='Blues', linewidths=0.5)

"""# Data Preparation"""

df_new.isnull().sum()

check_duplicates = df_new[df_new.duplicated()]
print(check_duplicates)

df_new_dropped = df_new.drop(['Volume'], axis=1)

from sklearn.decomposition import PCA
pca = PCA(n_components=1, random_state=123)
pca.fit(df_new_dropped[['Low', 'Open','High','Close', 'Adj Close']])

df_new_dropped['dimension'] = pca.transform(df_new_dropped.loc[:, ('Low', 'Open','High','Close', 'Adj Close')]).flatten()
df_new_dropped.drop(['Low', 'Open','High','Close', 'Adj Close'], axis=1, inplace=True)

df_new_dropped

df_new_dropped.index = df_new_dropped['Date']
df_new_dropped = df_new_dropped.drop(columns='Date')
df_new_dropped

# Splitting dataset
train_set = df_new_dropped[:int(len(df_new_dropped)*0.8) :]
test_set = df_new_dropped[int(len(df_new_dropped)*0.8):len(df_new_dropped) :]

train_set

test_set

def dataset_preparation(dataset, window):
    dframe = []
    label = []

    for i in range(len(dataset) - window - 1):
        data = dataset[i:(i + window), 0]
        dframe.append(data)
        label.append(dataset[i+window,0])
    return np.array(dframe), np.array(label)

# MinMax Scaling
minmax_scaler = MinMaxScaler()
scaled_train_set = minmax_scaler.fit_transform(train_set)
scaled_test_set = minmax_scaler.fit_transform(test_set)

# Prepare dataset with defined window size
x_train, y_train = dataset_preparation(scaled_train_set,80)
x_test, y_test = dataset_preparation(scaled_test_set,80)

# Reshaping
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],1))
x_test = np.reshape(x_test, (x_test.shape[0], x_train.shape[1],1))

print("\n----MinMax Scaled Data----")
print("All data : {}".format(len(scaled_train_set)))  

print("\n----Splitted Data----")
print("Train Set shapes : {} {}".format(x_train.shape, y_train.shape))
print("Test Set shapes : {} {}".format(x_test.shape, y_test.shape))

"""# Arsitektur Model"""

# Model Architecture
model = Sequential([
  LSTM(60),
  Dropout(0.5),
  Dense(32),
  Dense(1),
])

# Setting optimizer Adam Optimizer and learning rate
optimizer = Adam(learning_rate=0.01)

# Compile model with Huber and MAE (Mean Absolute Error) metrics
model.compile(
    metrics=["mae"],
    optimizer=optimizer,
    loss='mean_squared_error')

"""# Model Training"""

callbacks = EarlyStopping(
    min_delta=0.001,
    patience=5,
    restore_best_weights=True,
)

# Mae = (train_set['dimension'].max() - train_set['dimension'].min()) * (1/100)
# Mae = round(Mae,2)
# print(Mae)
# class myCallback(tf.keras.callbacks.Callback):
#   def on_epoch_end(self, epoch, logs={}):
#     if(logs.get('mae')<Mae and logs.get('val_mae')<Mae):
#       print("\nMAE dari model < 1% skala data : ", )
# callbacks_mae = myCallback()

# Training Model and save it to history variable.
history = model.fit(x_train,
                    y_train,
                    epochs=30,
                    validation_data=(x_test, y_test),
                    callbacks=[callbacks])

"""# Model Evaluation"""

# Create plot for mae and val_mae
figure, axes = plt.subplots(nrows=2, ncols=2)
figure.tight_layout(pad=3.0)

plt.subplot(1, 2, 1)
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'], 'r')
plt.title('Training and Testing MAE Value')
plt.xlabel('epoch')
plt.legend(['Training MAE', 'Testing MAE'], loc='upper right')

# Create plot for loss and val_loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Training and Testing Loss Value')
plt.xlabel('epoch')
plt.legend(['Training Loss', 'Testing Loss'], loc='upper right')

plt.show()

"""# Model Prediction"""

predict_test = model.predict(x_test)
predict_test = minmax_scaler.inverse_transform(predict_test)
actual_test = minmax_scaler.inverse_transform([y_test])

data_predict_and_actual = {'actual_value':actual_test[0], 'prediction_value' : predict_test[:,0]}

df_predict = pd.DataFrame(data_predict_and_actual)
df_predict['Date'] = df['Date'][:len(df_predict['actual_value'])]

df_predict.plot(x="Date" )
plt.ylabel('Harga Emas', size=12)

plt.xlabel('Date', size=12)
plt.xticks(rotation=30)

plt.legend(["Actual", "Prediction"], fontsize=15, loc='lower left')
plt.show();